{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pdf_to_text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk7jxIGUGwnd"
      },
      "source": [
        "!apt-get install python3-pypdf2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l9g4cG9VyxH"
      },
      "source": [
        "import PyPDF2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OzDWv3GVcsI"
      },
      "source": [
        "pdfFileObj = open('sample2.pdf','rb')\n",
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "number_of_pages = pdfReader.numPages\n",
        "count = 0\n",
        "text = \"\"\n",
        "while count < number_of_pages:\n",
        "    pageObj = pdfReader.getPage(count)\n",
        "    count +=1\n",
        "    text += pageObj.extractText()\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjTFfAWmNIId"
      },
      "source": [
        "import os \n",
        "import nltk \n",
        "import nltk.corpus\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "import pandas as pd \n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkC_txXCOEv3"
      },
      "source": [
        "h=[]\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "h.append(sentences)\n",
        "t_tokens=word_tokenize(text)\n",
        "t_tag=nltk.pos_tag(t_tokens)\n",
        "t_ner=ne_chunk(t_tag)\n",
        "L=t_ner\n",
        "print(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG2D2tMeVG1f"
      },
      "source": [
        "a=[]#contains noun\n",
        "l1=[]#sentences of noun\n",
        "l4=[]#tell if it's noun\n",
        "for s in sentences:\n",
        "  for word,pos in nltk.pos_tag(nltk.word_tokenize(str(s))):\n",
        "    if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
        "      a.append(word)\n",
        "      l1.append(s)\n",
        "      l4.append('noun')\n",
        "print(l1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BInInNzBZNA2"
      },
      "source": [
        "b=[]#contains verbs\n",
        "l2=[]#sentences of verb\n",
        "l3=[]#tell if it's verb\n",
        "for s in sentences:\n",
        "  for ww,pos in nltk.pos_tag(nltk.word_tokenize(str(s))):\n",
        "    if (pos == 'VB' or pos == 'VBD' or pos == 'VBG' or pos == 'VBN' or pos =='VBP' or pos =='VBZ'):\n",
        "      b.append(ww)\n",
        "      l2.append(s)\n",
        "      l3.append('verb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ3UE8Z5ZNiD"
      },
      "source": [
        "m1=l1+l2\n",
        "for i in m1:\n",
        "  print(i)\n",
        "  #top=nouns\n",
        "  #bottom=verbs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpBQ8AkOZQBb"
      },
      "source": [
        "m2=a+b\n",
        "for i in m2:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNo4pNXAZTgz"
      },
      "source": [
        "m3=l4+l3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LVmFN75ZZlj"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJTee2veZdXb"
      },
      "source": [
        "j=pd.DataFrame(m2,columns=['Words'])\n",
        "h=pd.DataFrame(m3,columns=['Noun/Verb'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9L9QUj6Zfcb"
      },
      "source": [
        "dict = {'Sentences': m1}\n",
        "df = pd.DataFrame(dict)\n",
        "df=pd.concat([df,j],axis=1)\n",
        "df=pd.concat([df,h],axis=1)\n",
        "df.to_csv('pdf_final.csv')\n",
        "df.head(120)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}